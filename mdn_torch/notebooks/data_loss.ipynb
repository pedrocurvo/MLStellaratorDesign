{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the loss as a mteric for the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch \n",
    "from qsc import Qsc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "import jax\n",
    "\n",
    "from MDNFullCovariance import MDNFullCovariance\n",
    "from utils import sample_output, check_criteria, run_qsc, round_nfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GStels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:  (10000, 20)\n"
     ]
    }
   ],
   "source": [
    "fname_gstels = '../data/GStels/GStels.csv'\n",
    "df_gstels = pd.read_csv(fname_gstels)\n",
    "\n",
    "# Sample 10000 to make it faster\n",
    "df_gstels = df_gstels.sample(10000, random_state=42)\n",
    "\n",
    "print(\"Shape of the dataset: \", df_gstels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGStels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:  (10000, 20)\n"
     ]
    }
   ],
   "source": [
    "fname_xgstels = '../data/XGStels/XGstels.csv'\n",
    "df_xgstels = pd.read_csv(fname_xgstels)\n",
    "\n",
    "# Sample 10000 to make it faster\n",
    "df_xgstels = df_xgstels.sample(10000, random_state=42)\n",
    "\n",
    "print(\"Shape of the dataset: \", df_xgstels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create model\n",
    "model_params = {\n",
    "    'input_dim': 10,\n",
    "    'output_dim': 10,\n",
    "    'num_gaussians': 62\n",
    "}\n",
    "model = MDNFullCovariance(**model_params).to(device)\n",
    "\n",
    "# Load models and mean_stds\n",
    "models = [ #\"../mdn_torch/models/MDNFullCovariance/2024_03_28_11_53_42.pth\",\n",
    "#           \"../mdn_torch/models/MDNFullCovariance/2024_03_30_02_40_44.pth\",\n",
    "#           \"../mdn_torch/models/MDNFullCovariance/2024_04_02_15_47_52.pth\",\n",
    "#           \"../mdn_torch/models/MDNFullCovariance/2024_04_03_10_03_21.pth\",\n",
    "#           \"../mdn_torch/models/MDNFullCovariance/2024_04_04_01_37_38.pth\",\n",
    "#           \"../mdn_torch/models/MDNFullCovariance/2024_04_04_13_41_19.pth\",\n",
    "            \"./models/MDNFullCovariance/2024_04_05_01_01_18.pth\"]\n",
    "mean_stds = [   # \"../mdn_torch/models/mean_std.pth\",\n",
    "                #  \"../mdn_torch/models/mean_std_2.pth\",\n",
    "                #  \"../mdn_torch/models/mean_std_3.pth\",\n",
    "                #  \"../mdn_torch/models/mean_std_4.pth\",\n",
    "                #  \"../mdn_torch/models/mean_std_5.pth\",\n",
    "                #  \"../mdn_torch/models/mean_std_combined.pth\",\n",
    "                \"./models/mean_std_5.pth\"]\n",
    "\n",
    "model_path = models[0]\n",
    "mean_std_path = mean_stds[0]\n",
    "\n",
    "# Load model\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "\n",
    "# Load mean_std\n",
    "mean_std = torch.load(mean_std_path, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss for GStels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features \n",
    "features = df_gstels.values.astype(np.float32)[:, 10:]\n",
    "\n",
    "# Labels\n",
    "labels = df_gstels.values.astype(np.float32)[:, 0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor\n",
    "features = torch.tensor(features).to(device)\n",
    "labels = torch.tensor(labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "features_norm = (features - mean_std['mean']) / mean_std['std']\n",
    "\n",
    "# Normalize labels\n",
    "labels_norm = (labels - mean_std['mean_labels']) / mean_std['std_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss: 100%|██████████| 10000/10000 [07:57<00:00, 20.95it/s, total_loss=0.274, axis_length=0.0525, iota=0.0255, max_elongation=0.00359, min_L_grad_B=0.299, min_R0=0.359, r_singularity=0.918, L_grad_grad_B=0.436, B20_variation=4.09e-5, beta=0.65, DMerc_times_r2=1.46e-8]   \n"
     ]
    }
   ],
   "source": [
    "glob_loss_total = 0\n",
    "glob_loss_axis_length = 0\n",
    "glob_loss_iota = 0\n",
    "glob_loss_max_elongation = 0\n",
    "glob_loss_min_L_grad_B = 0\n",
    "glob_loss_min_R0 = 0\n",
    "glob_loss_r_singularity = 0\n",
    "glob_loss_L_grad_grad_B = 0\n",
    "glob_loss_B20_variation = 0\n",
    "glob_loss_beta = 0\n",
    "glob_loss_DMerc_times_r2 = 0\n",
    "counted = 0\n",
    "from tqdm import tqdm\n",
    "progress_bar = tqdm(range(len(df_gstels)), desc=\"Calculating loss\")\n",
    "\n",
    "loss_fn = torch.nn.functional.huber_loss\n",
    "\n",
    "for i in progress_bar:\n",
    "    model_input = features_norm[i]\n",
    "    with torch.no_grad():\n",
    "        # Predict using model\n",
    "        model_outputs = model.getMixturesSample(model_input.unsqueeze(0), device)\n",
    "\n",
    "        # Denormalize output\n",
    "        model_outputs = model_outputs * mean_std[\"std_labels\"].to(device) + mean_std[\"mean_labels\"].to(device)\n",
    "\n",
    "        # Run Qsc\n",
    "        model_outputs = model_outputs.cpu().numpy()[0]\n",
    "        model_outputs = round_nfp(model_outputs)\n",
    "        \n",
    "        # Run Qsc\n",
    "        try: \n",
    "            qsc_output = run_qsc(model_outputs)\n",
    "            # Normalize qsc_values\n",
    "            qsc_output_normalized = (torch.tensor(qsc_output).to(device) - mean_std[\"mean\"].to(device)) / mean_std[\"std\"].to(device)\n",
    "            # Loss for each output\n",
    "            loss_axis_length = loss_fn(model_input[0], qsc_output_normalized[0]).item()\n",
    "            loss_iota = loss_fn(model_input[1], qsc_output_normalized[1]).item()\n",
    "            loss_max_elongation = loss_fn(model_input[2], qsc_output_normalized[2]).item()\n",
    "            loss_min_L_grad_B = loss_fn(model_input[3], qsc_output_normalized[3]).item()\n",
    "            loss_min_R0 = loss_fn(model_input[4], qsc_output_normalized[4]).item()\n",
    "            loss_r_singularity = loss_fn(model_input[5], qsc_output_normalized[5]).item()\n",
    "            loss_L_grad_grad_B = loss_fn(model_input[6], qsc_output_normalized[6]).item()\n",
    "            loss_B20_variation = loss_fn(model_input[7], qsc_output_normalized[7]).item()\n",
    "            loss_beta = loss_fn(model_input[8], qsc_output_normalized[8]).item()\n",
    "            loss_DMerc_times_r2 = loss_fn(model_input[9], qsc_output_normalized[9]).item()\n",
    "            loss_total = loss_fn(model_input, qsc_output_normalized).item()\n",
    "            # Add if loss is not inf or nan\n",
    "            if not np.isnan(loss_total) and not np.isinf(loss_total):\n",
    "                glob_loss_total += loss_total\n",
    "                glob_loss_axis_length += loss_axis_length\n",
    "                glob_loss_iota += loss_iota\n",
    "                glob_loss_max_elongation += loss_max_elongation\n",
    "                glob_loss_min_L_grad_B += loss_min_L_grad_B\n",
    "                glob_loss_min_R0 += loss_min_R0\n",
    "                glob_loss_r_singularity += loss_r_singularity\n",
    "                glob_loss_L_grad_grad_B += loss_L_grad_grad_B\n",
    "                glob_loss_B20_variation += loss_B20_variation\n",
    "                glob_loss_beta += loss_beta\n",
    "                glob_loss_DMerc_times_r2 += loss_DMerc_times_r2\n",
    "                counted += 1\n",
    "        except:\n",
    "            continue\n",
    "    # Update progress bar\n",
    "    progress_bar.set_postfix(\n",
    "        {\n",
    "            \"total_loss\": glob_loss_total / counted,\n",
    "            \"axis_length\": glob_loss_axis_length / counted,\n",
    "            \"iota\": glob_loss_iota / counted,\n",
    "            \"max_elongation\": glob_loss_max_elongation / counted,\n",
    "            \"min_L_grad_B\": glob_loss_min_L_grad_B / counted,\n",
    "            \"min_R0\": glob_loss_min_R0 / counted,\n",
    "            \"r_singularity\": glob_loss_r_singularity / counted,\n",
    "            \"L_grad_grad_B\": glob_loss_L_grad_grad_B / counted,\n",
    "            \"B20_variation\": glob_loss_B20_variation / counted,\n",
    "            \"beta\": glob_loss_beta / counted,\n",
    "            \"DMerc_times_r2\": glob_loss_DMerc_times_r2 / counted,\n",
    "        }\n",
    "    )\n",
    "    progress_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss for XGStels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features \n",
    "features = df_xgstels.values.astype(np.float32)[:, 10:]\n",
    "\n",
    "# Labels\n",
    "labels = df_xgstels.values.astype(np.float32)[:, 0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor\n",
    "features = torch.tensor(features).to(device)\n",
    "labels = torch.tensor(labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "features_norm = (features - mean_std['mean']) / mean_std['std']\n",
    "\n",
    "# Normalize labels\n",
    "labels_norm = (labels - mean_std['mean_labels']) / mean_std['std_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss: 100%|██████████| 10000/10000 [07:33<00:00, 22.05it/s, total_loss=0.208, axis_length=0.0398, iota=0.0274, max_elongation=0.000792, min_L_grad_B=0.342, min_R0=0.366, r_singularity=0.597, L_grad_grad_B=0.442, B20_variation=0.00556, beta=0.264, DMerc_times_r2=0.000128]\n"
     ]
    }
   ],
   "source": [
    "glob_loss_total = 0\n",
    "glob_loss_axis_length = 0\n",
    "glob_loss_iota = 0\n",
    "glob_loss_max_elongation = 0\n",
    "glob_loss_min_L_grad_B = 0\n",
    "glob_loss_min_R0 = 0\n",
    "glob_loss_r_singularity = 0\n",
    "glob_loss_L_grad_grad_B = 0\n",
    "glob_loss_B20_variation = 0\n",
    "glob_loss_beta = 0\n",
    "glob_loss_DMerc_times_r2 = 0\n",
    "counted = 0\n",
    "from tqdm import tqdm\n",
    "progress_bar = tqdm(range(len(df_gstels)), desc=\"Calculating loss\")\n",
    "for i in progress_bar:\n",
    "    model_input = features_norm[i]\n",
    "    with torch.no_grad():\n",
    "        # Predict using model\n",
    "        model_outputs = model.getMixturesSample(model_input.unsqueeze(0), device)\n",
    "\n",
    "        # Denormalize output\n",
    "        model_outputs = model_outputs * mean_std[\"std_labels\"].to(device) + mean_std[\"mean_labels\"].to(device)\n",
    "\n",
    "        # Run Qsc\n",
    "        model_outputs = model_outputs.cpu().numpy()[0]\n",
    "        model_outputs = round_nfp(model_outputs)\n",
    "\n",
    "        # Loss \n",
    "        loss_fn = torch.nn.functional.huber_loss\n",
    "        \n",
    "        # Run Qsc\n",
    "        try: \n",
    "            qsc_output = run_qsc(model_outputs)\n",
    "            # Normalize qsc_values\n",
    "            qsc_output_normalized = (torch.tensor(qsc_output).to(device) - mean_std[\"mean\"].to(device)) / mean_std[\"std\"].to(device)\n",
    "            # Loss for each output\n",
    "            loss_axis_length = loss_fn(model_input[0], qsc_output_normalized[0]).item()\n",
    "            loss_iota = loss_fn(model_input[1], qsc_output_normalized[1]).item()\n",
    "            loss_max_elongation = loss_fn(model_input[2], qsc_output_normalized[2]).item()\n",
    "            loss_min_L_grad_B = loss_fn(model_input[3], qsc_output_normalized[3]).item()\n",
    "            loss_min_R0 = loss_fn(model_input[4], qsc_output_normalized[4]).item()\n",
    "            loss_r_singularity = loss_fn(model_input[5], qsc_output_normalized[5]).item()\n",
    "            loss_L_grad_grad_B = loss_fn(model_input[6], qsc_output_normalized[6]).item()\n",
    "            loss_B20_variation = loss_fn(model_input[7], qsc_output_normalized[7]).item()\n",
    "            loss_beta = loss_fn(model_input[8], qsc_output_normalized[8]).item()\n",
    "            loss_DMerc_times_r2 = loss_fn(model_input[9], qsc_output_normalized[9]).item()\n",
    "            loss_total = loss_fn(model_input, qsc_output_normalized).item()\n",
    "            # Add if loss is not inf or nan\n",
    "            if not np.isnan(loss_total) and not np.isinf(loss_total):\n",
    "                glob_loss_total += loss_total\n",
    "                glob_loss_axis_length += loss_axis_length\n",
    "                glob_loss_iota += loss_iota\n",
    "                glob_loss_max_elongation += loss_max_elongation\n",
    "                glob_loss_min_L_grad_B += loss_min_L_grad_B\n",
    "                glob_loss_min_R0 += loss_min_R0\n",
    "                glob_loss_r_singularity += loss_r_singularity\n",
    "                glob_loss_L_grad_grad_B += loss_L_grad_grad_B\n",
    "                glob_loss_B20_variation += loss_B20_variation\n",
    "                glob_loss_beta += loss_beta\n",
    "                glob_loss_DMerc_times_r2 += loss_DMerc_times_r2\n",
    "                counted += 1\n",
    "        except:\n",
    "            continue\n",
    "    # Update progress bar\n",
    "    progress_bar.set_postfix(\n",
    "        {\n",
    "            \"total_loss\": glob_loss_total / counted,\n",
    "            \"axis_length\": glob_loss_axis_length / counted,\n",
    "            \"iota\": glob_loss_iota / counted,\n",
    "            \"max_elongation\": glob_loss_max_elongation / counted,\n",
    "            \"min_L_grad_B\": glob_loss_min_L_grad_B / counted,\n",
    "            \"min_R0\": glob_loss_min_R0 / counted,\n",
    "            \"r_singularity\": glob_loss_r_singularity / counted,\n",
    "            \"L_grad_grad_B\": glob_loss_L_grad_grad_B / counted,\n",
    "            \"B20_variation\": glob_loss_B20_variation / counted,\n",
    "            \"beta\": glob_loss_beta / counted,\n",
    "            \"DMerc_times_r2\": glob_loss_DMerc_times_r2 / counted,\n",
    "        }\n",
    "    )\n",
    "    progress_bar.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
